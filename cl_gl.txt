Oh dear.  The way cl_gl works is that you allocate a buffer with the GL, and then you bind it to a CL context.  This pretty much necessitates pre-making the buffers, because the GL context is bound to the main thread.
Therefore.
 - Use clGetGLContextInfoKHR() to determine which CL device matches the graphics device.
 - To begin with, have AFK_Computer hold a single CL context and queue, and lock across enqueueing things to the CL queue.  If this appears to be really hurting, try multiple contexts -- but I'll need to map the big buffers (below!) to each context, which might not work well!
 - Make large GL buffer for all the landscape tile vertex data.  (Work out the amount of RAM available on the graphics device and divide it up suitably.)
 - Heapify that GL buffer: each landscape tile will take the same amount of data.  I will need a queue of free slots.
 - Bind a matchingly large CL buffer to the GL buffer.  It will be READ_WRITE.
 - Make a matching index buffer (GL and CL too).  (A happy coincidence of this is that I can draw the whole landscape in one go.  Well, I need to try.  It might turn out that I can't simultaneously read and write portions of a large buffer between GL and CL, in which case I'll need to pre-allocate one buffer per cached tile, instead.)
 - Change the landscape tile cache to wrap up all of that and abstract it.  Now, instead of caching the geometry itself, I'll be caching offsets into the GL/CL buffers.
 - Change the landscape tile display code to invoke a large base-vertex-and-multi-and-whatever-you-call it draw across that buffer.  I'll need to feed in a stream of exactly what stuff to draw as input.  (That stream goes across the PCI-E.  The whole landscape geometry should stay on the GPU side.)
 - Change the landscape tile creation code to bake it in OpenCL, straight into said big shared vertex and index heap-buffers.  (I'll want to back out the whole landscape displacement thingy and do a flat one first, to make sure I've transliterated everything correctly.)
 - After all that is working, do the same thing to the shapes.
 - This is enforced OpenCL -- all GPU-side data stays GPU-side, I'll actually need a compute kernel to transfer small bits of it to the CPU if it needs access.  It also might make my threading model redundant as I end up needing to synchronise almost everything the workers do, which would make me sad, but never mind, eh?

Look above -- no.  I probably don't want to make *one* big vertex buffer, it's going to go wrong -- the glMultiDraw* calls don't look flexible enough to handle the inevitable amounts of variation between rendered tiles.  Also, I want to have a reasonably busy CL queue in order to allow the GPU to get busy as quickly as possible during a frame: batching all the tile baking into a single call will delay everything.
Rather, try counting the number of tiles I want to be able to cache, and pre-allocating one buffer for each (CL and GL).
So reserve one (vertex, index) buffer per potential tile first.  (Make the eviction target for the cache something like 1/2 the buffer count, because if the heap allocate function actually runs out of free buffers in the queue, it's going to throw an exception).

Note: I think I may need to govern the OpenGL context myself (which GLUT doesn't appear to let me do).  To initialise cl_gl, I need to give it an OpenGL "share group", which I'm failing to find documentation on.  Maybe read up on how to use GLX?  (and I'd need WGL to make it run on Windows too...).  Hmm.  See SA09_GL_interop.pdf
 - If I'm going to convert to GLX, then sadly, I think I first need to write a program that displays an empty window (with X) and runs a main loop and responds to keystroke events (echoing them to the terminal).  Because that entire thing will need rewriting.  :-(
 - Also see sa10-dg-opencl-gl-interop.pdf


